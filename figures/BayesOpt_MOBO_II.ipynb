{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5268353-3435-44c7-88e5-a3b1326b5e44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multi-objective Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070e7b-2268-4be5-b3fc-b197a0669dcd",
   "metadata": {
    "id": "e3070e7b-2268-4be5-b3fc-b197a0669dcd"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ArISd1WjiHjr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArISd1WjiHjr",
    "outputId": "eb84dba5-2329-4257-9d4b-5376609c6f7c"
   },
   "outputs": [],
   "source": [
    "# When running in google colab\n",
    "#pip install cobra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "SNh5PxaHiYGd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNh5PxaHiYGd",
    "outputId": "c2e1896e-0f3a-43c7-9e01-6f78fbe60235"
   },
   "outputs": [],
   "source": [
    "# When running in google colab\n",
    "#pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb93be1-e79c-469d-a533-3820e3e0b15e",
   "metadata": {
    "id": "5eb93be1-e79c-469d-a533-3820e3e0b15e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BayesOpt\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils.transforms import unnormalize, normalize # for normalising media components\n",
    "# sampler\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "# ACQUISITION FUNCTION\n",
    "# for qPAREGO\n",
    "from botorch.optim.optimize import optimize_acqf_list \n",
    "from botorch.acquisition.logei import qLogExpectedImprovement\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "# for q(Log)NEHVI\n",
    "from botorch.optim import optimize_acqf \n",
    "from botorch.acquisition.multi_objective.logei import qLogNoisyExpectedHypervolumeImprovement # for qNEHVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ab0f7-96ad-4c14-a098-54d20514c50e",
   "metadata": {
    "id": "d64ab0f7-96ad-4c14-a098-54d20514c50e"
   },
   "source": [
    "### Helper Functions & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec471a-b284-4ed5-a01e-b24221e85feb",
   "metadata": {
    "id": "99ec471a-b284-4ed5-a01e-b24221e85feb"
   },
   "outputs": [],
   "source": [
    "# Plotting functions to be used across notebooks\n",
    "%run HelperFunctions_MOBO_II.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de275776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for .py version\n",
    "#from Plotting_MOBO_II import *\n",
    "#from BayesOpt_MOBO_II import *\n",
    "#from HelperFunctions_MOBO_II import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f51bc6-cf50-49d7-9b66-a4ee10c72bb6",
   "metadata": {},
   "source": [
    "# BayesOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa1b3e",
   "metadata": {},
   "source": [
    "## Next Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097eef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: ref-point for qNEHVI -> Reference Points\n",
    "def  find_next_candidates(\n",
    "        medium_tensors_normalised_stacked,\n",
    "        growth_tensors,\n",
    "        cost_tensors, \n",
    "        production_tensors = None,\n",
    "        opt_objective = \"growth-cost\",\n",
    "        AF_type = \"qPAREGO\",\n",
    "        n_candidates = 5\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Finds the next medium composition for which to evaluate cost and optimal growth rate\n",
    "    * initialises botorch model (list of SingleTaskGP) and mll\n",
    "    * fits model using mll\n",
    "    * sets up SobolQMCNormalSampler to sample from posterior\n",
    "    * when qPAREGO is to be used\n",
    "        * computes posterior mean\n",
    "        * initialises list of acquisition functions (one per candidate of batch)\n",
    "        * for each candidate (batch size)\n",
    "            * uses chebyshev_scalarization to create a vector representation of the chosen objectives\n",
    "            * defines qLogExpectedImprovement acquisition function and appends to list\n",
    "        * finds all candidates depending on the acquisition functions\n",
    "    * when qNEHVI is to be used\n",
    "        * set re-point\n",
    "        * initialise qLogNoisyExpectedHypervolumeImprovement acquisition function\n",
    "        * find n candidates using optimize_acqf\n",
    "\n",
    "    PARAMETERS\n",
    "    * medium_tensors_normalised_stacked - tensor - all medium compositions previously evaluated 0-1 normalised, \n",
    "    stored as tensors (in order)\n",
    "    * growth_tensors - tensor - corresponding growth rates\n",
    "    * cost_tensors - tensor - corresponding medium costs\n",
    "    * production_tensors - tensor - corresponding production rates\n",
    "    * opt-objective - string - the (multi-)objective for which to find the optimal medium composition\n",
    "    * AF_Type - string - which acquisition function to use (qPAREGO or qNEHVI)\n",
    "    * n_candidates - integer - how many candidates to find at once\n",
    "\n",
    "    RETURNS\n",
    "    * candidates - tensor - a tensors with n_candidates 0-1 normalised medium compositions to be tested\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    '''parameters and conversion to tensors; normalisation of medium composition to (0,1)'''\n",
    "    MC_SAMPLES = 256 #256 # Number of Monte Carlo samples in SobolQMCNormalSampler\n",
    "    n_components = medium_tensors_normalised_stacked.size()[1] # of medium components\n",
    "    standard_bounds = torch.tensor([[0.0] * n_components,\n",
    "                                    [1.0] * n_components]).to(**tkwargs) # normalised bounds for medium composition\n",
    "    # large values -> slower but possibly better accuracy\n",
    "    NUM_RESTARTS =  5 #10 # Number of restarts for acquisition function optimisation\n",
    "    RAW_SAMPLES = 512 # 1024 # Number of raw samples for initialisation of acquisition optimisation\n",
    "\n",
    "\n",
    "    '''finding the new candidate'''\n",
    "    # initialise GP model and marginal likelihood (mll)\n",
    "    mll, model = initialise_model(\n",
    "        medium_tensors_normalised_stacked,\n",
    "        growth_tensors,\n",
    "        opt_objective, \n",
    "        cost_tensors, \n",
    "        production_tensors)\n",
    "    \n",
    "    fit_gpytorch_mll(mll) # Fit the model using the maximum marginal likelihood\n",
    "    # Set up a Sobol quasi-Monte Carlo sampler for sampling from the posterior\n",
    "    # The sample_shape should correspond to the shape of the posterior samples needed\n",
    "    # https://botorch.readthedocs.io/en/latest/sampling.html#botorch.sampling.normal.SobolQMCNormalSampler\n",
    "    sampler = SobolQMCNormalSampler(sample_shape = torch.Size([MC_SAMPLES]), seed = MC_SAMPLES)\n",
    "\n",
    "\n",
    "    if AF_type == \"qPAREGO\":\n",
    "        # Compute the posterior mean for the given medium_tensors_stacked using the model\n",
    "        with torch.no_grad():\n",
    "            posterior = model.posterior(medium_tensors_normalised_stacked).mean\n",
    "\n",
    "        acq_fun_list = [] # List to hold acquisition functions for each candidate\n",
    "        # Loop to generate acquisition functions for each candidate\n",
    "        for _ in range(n_candidates):\n",
    "            # Sample weights from the simplex for Chebyshev scalarization\n",
    "            weights = sample_simplex(2, **tkwargs).squeeze() # using 2 weights for scalarization (growth and cost or production)\n",
    "\n",
    "            # Compute the scalarised objective values for all the training points\n",
    "            # Sample weights from the simplex for Chebyshev scalarization\n",
    "            if opt_objective == \"growth-cost\":\n",
    "                scalarized_objective_values = (\n",
    "                    weights[0] * growth_tensors + \n",
    "                    weights[1] * cost_tensors)\n",
    "            elif opt_objective == \"growth-production\":\n",
    "                scalarized_objective_values = (\n",
    "                    weights[0] * growth_tensors + \n",
    "                    weights[1] * production_tensors)\n",
    "            elif opt_objective == \"production-cost\":\n",
    "                scalarized_objective_values = (\n",
    "                    weights[0] * production_tensors + \n",
    "                    weights[1] * cost_tensors)\n",
    "            elif opt_objective == \"growth-production-cost\":\n",
    "                # using 3 weights for scalarization (growth, production, and cost)\n",
    "                weights = sample_simplex(3, **tkwargs).squeeze()\n",
    "                scalarized_objective_values = (\n",
    "                    weights[0] * growth_tensors + \n",
    "                    weights[1] * production_tensors +\n",
    "                    weights[2] * cost_tensors)\n",
    "\n",
    "            # Find the best observed scalarized objective value\n",
    "            best_f = scalarized_objective_values.max().item()\n",
    "\n",
    "            # Define objective\n",
    "            objective = GenericMCObjective(\n",
    "                get_chebyshev_scalarization(weights, posterior)\n",
    "            )\n",
    "\n",
    "            # Define the acquisition function using quasi Monte Carlo EI\n",
    "            acq_fun = qLogExpectedImprovement(\n",
    "                model = model, # List of SingleTastk GP\n",
    "                best_f = best_f, # best objective value observed so far - replaces X_baseline in Noisy version\n",
    "                sampler = sampler, # SobolQMCNormalSampler\n",
    "                objective = objective, # combination of objectives - Chebyshev scalarization\n",
    "            )\n",
    "            acq_fun_list.append(acq_fun)\n",
    "\n",
    "        candidates, _ = optimize_acqf_list(\n",
    "            acq_function_list = acq_fun_list,  # List of acquisition functions to optimise\n",
    "            bounds = standard_bounds, # The normalised bounds for optimisation\n",
    "            num_restarts = NUM_RESTARTS, # Number of restarts for optimisation\n",
    "            raw_samples = RAW_SAMPLES, # Number of raw samples for initialisation (?)\n",
    "            options = {\"batch_limit\": 10, \"maxiter\": 200,} # Options for acquisition function optimisation\n",
    "        )\n",
    " \n",
    "    elif AF_type == \"qNEHVI\":\n",
    "        '''REFERENCE POINT'''\n",
    "        # set based on domain knowledge\n",
    "        # should be set slightly worse than the current Pareto Front estimate\n",
    "        # it should be possible to find datapoints dominating both variables\n",
    "        if opt_objective == \"growth-cost\":\n",
    "            # print(growth_tensors.max(), cost_tensors.max(), sep = \"\\n\")\n",
    "            ref_point = torch.stack([\n",
    "                torch.tensor(0.6), \n",
    "                torch.tensor(200)\n",
    "                ])\n",
    "        elif opt_objective == \"growth-production\":\n",
    "            ref_point = torch.stack([\n",
    "                torch.tensor(0.6), \n",
    "                torch.tensor(0.0002)\n",
    "                ])\n",
    "        elif opt_objective == \"production-cost\":\n",
    "            ref_point = torch.stack([\n",
    "                torch.tensor(0.0002), \n",
    "                torch.tensor(200)\n",
    "                ])\n",
    "        elif opt_objective == \"growth-production-cost\":\n",
    "            ref_point = torch.stack([\n",
    "                torch.tensor(0.6), \n",
    "                torch.tensor(0.0002),\n",
    "                torch.tensor(200)\n",
    "                ])\n",
    "\n",
    "        # partition non-dominated space into disjoint rectangles\n",
    "        # model has been initialised based on optimisation goal\n",
    "        acq_func = qLogNoisyExpectedHypervolumeImprovement(\n",
    "            model = model,\n",
    "            ref_point = ref_point.tolist(),  # use known reference point\n",
    "            X_baseline = medium_tensors_normalised_stacked, # normalize(train_x, problem.bounds)\n",
    "            prune_baseline = True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "            sampler = sampler\n",
    "            )\n",
    "        \n",
    "        # optimize\n",
    "        candidates, _ = optimize_acqf(\n",
    "            acq_function = acq_func, # qNEHVI\n",
    "            bounds = standard_bounds, # The normalised bounds for optimisation\n",
    "            q = n_candidates,\n",
    "            num_restarts = NUM_RESTARTS, # Number of restarts for optimisation\n",
    "            raw_samples = RAW_SAMPLES, # used for intialization heuristic\n",
    "            options = {\"batch_limit\": 20, \"maxiter\": 200}, # Options for acquisition function optimisation\n",
    "            sequential = True,\n",
    "            )\n",
    "        \n",
    "    \"\"\"     \n",
    "    # SANITY CHECK: Get model predictions at selected candidate locations\n",
    "    # does the model believe that it's doing well?\n",
    "    with torch.no_grad():\n",
    "        growth_pred = model.models[0].posterior(candidates).mean.squeeze()\n",
    "        cost_pred_transformed = model.models[1].posterior(candidates).mean.squeeze()\n",
    "    print(\"Candidate Prediction Sanity Check:\")\n",
    "    print(\"Predicted Growth:\", growth_pred)\n",
    "    print(\"Predicted Cost (Transformed):\", cost_pred_transformed)\n",
    "    \"\"\"\n",
    "    \n",
    "    return candidates # candidate_tensor_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e81206-6abd-4ea8-96a8-ecd1fc41e3c3",
   "metadata": {
    "id": "b2e81206-6abd-4ea8-96a8-ecd1fc41e3c3"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebade7a-1f07-4932-a448-5ed3acba40c8",
   "metadata": {
    "id": "3ebade7a-1f07-4932-a448-5ed3acba40c8"
   },
   "outputs": [],
   "source": [
    "# TODO: Use the dedicated function (for medium conversion)\n",
    "# TODO: check that model.medium and costs have the same number of entries\n",
    "def media_BayesOpt(\n",
    "        MetModel, \n",
    "        medium = None, \n",
    "        bounds = None, \n",
    "        costs = None,\n",
    "        opt_objective = \"growth-cost\",\n",
    "        biomass_objective = None,\n",
    "        production_objective = None,\n",
    "        AF_type = \"qPAREGO\",\n",
    "        n_start = 5,\n",
    "        n_iter = 50,\n",
    "        n_candidates = 5,\n",
    "        model_objective = None\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Performs medium optimisation for various objectives: trade-off between \n",
    "    * growth rate and medium cost\n",
    "    * growth rate and production rate\n",
    "    * production rate and medium cost\n",
    "    * growth rate, production rate and medium cost\n",
    "\n",
    "    1. Sets default values for medium, bounds and costs if not provided by the user\n",
    "    2. Performs optimisation n_iter (default = 50) times\n",
    "        1. calls generate_initial_data(args) to generate initial data points\n",
    "        2. finds new candidate medium calling find_next_candidate(args)\n",
    "        3. evaluates new medium for growth rate and costs\n",
    "        4. keeps all values\n",
    "    3. returns optimal composition alongside corresponding cost, growth, and cost-growth trade-off\n",
    "\n",
    "    PARAMETERS:\n",
    "    * MetModel - COBRApy model - the metabolic model to be evaluated\n",
    "    * medium - dictionary - the medium composition of that model; if not provided defaults to default medium provided by CobraPy\n",
    "    * bounds - dictionary - upper and lower bounds for the values the medium components are allowed to take,\n",
    "    determines the search space; if not provided defaults to 0, and current medium value\n",
    "    * costs - dictionary - the (monetary) cost of each component; if not provided defaults to unit costs\n",
    "    * opt_objective - string - indicates what is to be optimised\n",
    "    * biomass_objective - string - the name of the biomass reaction of the chosen model\n",
    "    * production_objective - string - the name of the producing reaction to be maximised of the chosen model\n",
    "    * AF_Type - string - which acquisition function to use (qPAREGO or qNEHVI)\n",
    "    * n_start - integer - how many random media compositions are to be created to set up the BayesOpt\n",
    "    * n_iter - integer - how many candidate medium compositions should be found and evaluated\n",
    "    * n_candidates - integer - how many candidates to find at once\n",
    "    * model_objective - CPBRApy objective - what is set as the objective of the modelled organisms, used in FBA\n",
    "\n",
    "    RETURNS:\n",
    "    A dictionary containing\n",
    "    * \"medium list\" - a list of all evaluated medium compositions\n",
    "    * \"medium component bounds\" - a dictionary with the upper and lower bounds of each medium components\n",
    "    * \"medium component costs\" - a dictionary with the cost of each medium component\n",
    "    * \"growth rate tensors\" - a tensor with corresponding growth rates\n",
    "    * \"cost tensors\" - a tensor with corresponding total medium costs\n",
    "    * \"production tensors\" - a tensor with corresponding production rates\n",
    "    * \"is pareto\" -\n",
    "    * \"optimisation objective\" - the objective with which the algorithm was run\n",
    "    * \"biomass objective\" - the biomass function to be optimised\n",
    "    * \"production objective\" - the production flux to be optimised\n",
    "    * \"model objective\" - the COBRApy objective of the model that was used\n",
    "    * \"AF_type\" - the acquisition function that was used\n",
    "    * \"n_start\" - number of random start points\n",
    "    * \"n_iter\" - number of iterations\n",
    "    * \"n_candidates\" - batch size\n",
    "    \"\"\"\n",
    "\n",
    "    '''TEST VALIDITY OF ARGUMENTS'''\n",
    "    # AF_Type\n",
    "    valid_AF_types = {\"qPAREGO\", \"qNEHVI\"}\n",
    "    if AF_type not in valid_AF_types:\n",
    "        raise ValueError(f\"AF_type must be one of {valid_AF_types}, but got '{AF_type}'\")\n",
    "    \n",
    "    # opt_objective\n",
    "    valid_opt_objective = {\"growth-cost\", \"growth-production\", \"production-cost\", \"growth-production-cost\"}\n",
    "    if opt_objective not in valid_opt_objective:\n",
    "        raise ValueError(f\"opt_objective must be one of {valid_opt_objective}, but got '{opt_objective}'\")\n",
    "\n",
    "    '''INITIALISE'''\n",
    "    # Set default values for medium, boundaries and costs\n",
    "    if medium is None:\n",
    "        medium = MetModel.medium  # Default medium to model.medium if not provided\n",
    "    if bounds is None:\n",
    "        # if no bounds are provided, set the lower limit to 0 and upper to the value in medium\n",
    "        bounds = {key: (0, medium[key]) for key in medium.keys()}\n",
    "    if costs is None:\n",
    "        # set unit costs if no costs are provided\n",
    "        costs = {key: 1 for key in medium.keys()}\n",
    "    # TODO: check that model.medium and costs have the same number of entries\n",
    "\n",
    "    # if a model_objective is given, set it \n",
    "    if model_objective:\n",
    "        MetModel.objective = model_objective\n",
    "\n",
    "    '''GET RANDOM INITIAL DATA POINTS'''\n",
    "    # generate n_start initial data points (parameters and corresponding cost + growth rate)\n",
    "    initial_para, initial_growth, initial_production, initial_cost = generate_initial_data(\n",
    "        MetModel, medium, bounds, costs,\n",
    "        n_samples = n_start, opt_objective = opt_objective, \n",
    "        biomass_objective = biomass_objective, production_objective = production_objective)\n",
    "    \n",
    "    medium_list = initial_para # list of dictonaries\n",
    "    medium_keys = medium_list[-1].keys() # extract keys from medium_list\n",
    "    growth_tensors = initial_growth\n",
    "    growth_tensors_normalised = normalise_1Dtensors(growth_tensors)\n",
    "    production_tensors = initial_production\n",
    "    production_tensors_normalised = normalise_1Dtensors(production_tensors)\n",
    "    cost_tensors = initial_cost # tensor\n",
    "    cost_tensors_normalised = normalise_1Dtensors(cost_tensors) # min-max normalised\n",
    "    is_pareto = []\n",
    "    \n",
    "    '''CONVERT MEDIUM_LIST TO TENSOR'''\n",
    "    # TODO: Use the dedicated function\n",
    "    # convert bounds from dictionary to tensor\n",
    "    bounds_tensor = torch.tensor(list(bounds.values()), dtype=torch.double).to(**tkwargs) # [x, 2]\n",
    "    # Stack the lower and upper bounds to match the expected format\n",
    "    bounds_tensors_stacked = torch.stack([bounds_tensor[:, 0], bounds_tensor[:, 1]], dim=0)\n",
    "\n",
    "    # normalise medium composition\n",
    "    medium_tensors_normalised = [] # initialise empty list\n",
    "    for m in range(len(medium_list)):\n",
    "        # transform current medium to tensor\n",
    "        medium_m = medium_list[m]\n",
    "        medium_m_tensor = torch.tensor(list(medium_m.values()), dtype=torch.double).to(**tkwargs) # [x]\n",
    "        # normalise medium composition using the bounds\n",
    "        normalised_medium_m = normalize(medium_m_tensor, bounds_tensors_stacked)\n",
    "        # Append the normalized tensor to the list\n",
    "        medium_tensors_normalised.append(normalised_medium_m)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Growth:\", growth_tensors, \n",
    "          \"Cost:\", cost_tensors, \n",
    "          \"Cost normalised:\", cost_tensors_normalised,\n",
    "          \"Production:\", production_tensors,\n",
    "          \"Production normalised:\", production_tensors_normalised,\n",
    "          sep = \"\\n\")\n",
    "    \"\"\"\n",
    "    '''MAIN LOOP'''\n",
    "    for i in range(n_iter):\n",
    "        # Stack the list of tensors along a new dimension (dim=0) -> single tensor\n",
    "        medium_tensors_normalised_stacked = torch.stack(medium_tensors_normalised, dim = 0) # normalised\n",
    "        # Use BayesOpt to change medium\n",
    "        '''Need to pass normalised cost and production for qNEHVI, especially'''        \n",
    "        if AF_type == \"qPAREGO\":\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked,\n",
    "                growth_tensors,\n",
    "                (1 - cost_tensors_normalised), # because costs should be minimised but function maximises\n",
    "                production_tensors = production_tensors_normalised,\n",
    "                opt_objective = opt_objective,\n",
    "                AF_type = AF_type,\n",
    "                n_candidates = n_candidates\n",
    "                )\n",
    "        elif AF_type == \"qNEHVI\":\n",
    "            \"\"\"\n",
    "            # all y's are 0-1 normalised\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked,\n",
    "                growth_tensors_normalised,\n",
    "                (1 - cost_tensors_normalised), # because costs should be minimised but function maximises\n",
    "                production_tensors = production_tensors_normalised,\n",
    "                opt_objective = opt_objective,\n",
    "                AF_type = AF_type,\n",
    "                n_candidates = n_candidates\n",
    "                )\n",
    "            \"\"\"\n",
    "            max_cost = cost_tensors.max()\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked,\n",
    "                growth_tensors,\n",
    "                (max_cost - cost_tensors), # because costs should be minimised but function maximises\n",
    "                production_tensors = production_tensors,\n",
    "                opt_objective = opt_objective,\n",
    "                AF_type = AF_type,\n",
    "                n_candidates = n_candidates\n",
    "                )\n",
    "\n",
    "        # if n_candidates > 1, each candidate needs to be evaluated individually\n",
    "        # TODO: parallelise?\n",
    "        for candidate_tensor_normalised in candidates_tensor_normalised:\n",
    "                \n",
    "            # unnormlise new candidate\n",
    "            candidate_tensor_unnormalised = unnormalize(candidate_tensor_normalised, bounds_tensors_stacked)\n",
    "            # convert back to dictionary            \n",
    "            candidate_medium = convert_to_dict(candidate_tensor_unnormalised, medium_keys)\n",
    "                \n",
    "            # for new medium compute new values\n",
    "            cost_tot = calc_cost_tot(costs, candidate_medium) # tensor\n",
    "            MetModel.medium = candidate_medium # reassign medium\n",
    "            # perform FBA\n",
    "            solution = MetModel.optimize()\n",
    "            # extract growth rate\n",
    "            # TODO: find a solution for when biomass_objective = None\n",
    "            FBA_growth = solution.fluxes[biomass_objective]\n",
    "            # some model compositions lead to FBA returns NaN or negative numbers\n",
    "            # to avoid them from breaking the algorithm, set growth to zero\n",
    "            if (np.isnan(FBA_growth) or FBA_growth < 0):\n",
    "                FBA_growth = 0\n",
    "\n",
    "            if opt_objective == \"growth-cost\":\n",
    "                FBA_production = -1\n",
    "                \n",
    "            elif (opt_objective == \"growth-production\" or \n",
    "                  opt_objective == \"production-cost\" or\n",
    "                  opt_objective == \"growth-production-cost\"):\n",
    "                FBA_production = solution.fluxes[production_objective]\n",
    "                if (np.isnan(FBA_production) or FBA_production < 0):\n",
    "                    FBA_production = 0\n",
    "\n",
    "            '''APPEND RESULTS TO TENSORS AND NORMALISE'''\n",
    "            # medium lists\n",
    "            medium_list.append(candidate_medium)\n",
    "            medium_tensors_normalised.append(candidate_tensor_normalised)\n",
    "            # growth\n",
    "            FBA_growth_tensor = torch.tensor([FBA_growth], dtype=torch.double).to(**tkwargs)\n",
    "            growth_tensors = torch.cat((growth_tensors, FBA_growth_tensor), dim = 0)  # Concatenate along dimension 0\n",
    "            growth_tensors_normalised = normalise_1Dtensors(growth_tensors)\n",
    "            # production\n",
    "            FBA_production_tensor = torch.tensor([FBA_production], dtype = torch.double).to(**tkwargs)\n",
    "            production_tensors = torch.cat((production_tensors, FBA_production_tensor), dim = 0)\n",
    "            production_tensors_normalised = normalise_1Dtensors(production_tensors)\n",
    "            # cost\n",
    "            cost_tensors = torch.cat((cost_tensors, cost_tot), dim = 0)  # Concatenate along dimension 0 (1D tensors)\n",
    "            cost_tensors_normalised = normalise_1Dtensors(cost_tensors) # new min-max normalisation\n",
    "                \n",
    "        if ((i+1)%10 == 0):\n",
    "            print(\"Iteration:\\t\", i+1)\n",
    "\n",
    "    '''FIND POINTS ON PARETO FRONT'''\n",
    "    # Find all points on pareto front and return them     \n",
    "    # Stack all (two/three) objectives into a single 2D tensor\n",
    "    # rows: candidates; columns: objectives\n",
    "    # is_non_dominated assumes maximisation -> negate costs\n",
    "    if opt_objective == \"growth-cost\":\n",
    "        y = torch.stack((growth_tensors, cost_tensors*(-1)), dim = 1)\n",
    "    elif opt_objective == \"growth-production\":\n",
    "        y = torch.stack((growth_tensors, production_tensors), dim = 1)\n",
    "    elif opt_objective == \"production-cost\":\n",
    "        y = torch.stack((production_tensors, cost_tensors*(-1)), dim = 1)\n",
    "    elif opt_objective == \"growth-production-cost\":\n",
    "        y = torch.stack((growth_tensors, production_tensors, cost_tensors*(-1)), dim = 1)\n",
    "\n",
    "    # Compute non-dominated (Pareto front) points; i.e. optimal trade.offs\n",
    "    is_pareto = is_non_dominated((y).to(**tkwargs))\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Growth:\", growth_tensors, \n",
    "          \"Cost:\", cost_tensors, \n",
    "          \"Cost normalised:\", cost_tensors_normalised,\n",
    "          \"Production:\", production_tensors,\n",
    "          \"Production normalised:\", production_tensors_normalised,\n",
    "          sep = \"\\n\")\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"medium list\" : medium_list, \n",
    "        \"medium component bounds\" : bounds,\n",
    "        \"medium component costs\" : costs, \n",
    "        \"growth rate tensors\" : growth_tensors,\n",
    "        \"production tensors\" : production_tensors, \n",
    "        \"cost tensors\" : cost_tensors,\n",
    "        \"is pareto\" : is_pareto,\n",
    "        \"optimisation objective\" : opt_objective,\n",
    "        \"biomass objective\" : biomass_objective,\n",
    "        \"production objective\" : production_objective,\n",
    "        \"model objective\" : model_objective,\n",
    "        \"AF_type\" : AF_type,\n",
    "        \"n_start\" : n_start,\n",
    "        \"n_iter\" : n_iter,\n",
    "        \"n_candidates\" : n_candidates\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "374ed588-3595-4c5a-b344-c1e2d36fb6b3"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Bayesian-opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
